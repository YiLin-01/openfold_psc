[2024-05-29 17:14:24,214] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
orig_dtype: torch.float32
Using /jet/home/ylinf/.cache/torch_extensions/py39_cu116 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /jet/home/ylinf/.cache/torch_extensions/py39_cu116/evoformer_attn/build.ninja...
Building extension module evoformer_attn...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module evoformer_attn...
Time to load evoformer_attn op: 0.4720470905303955 seconds
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
[model !!!!!!!] AlphaFold(
  (input_embedder): InputEmbedder(
    (linear_tf_z_i): Linear(in_features=22, out_features=128, bias=True)
    (linear_tf_z_j): Linear(in_features=22, out_features=128, bias=True)
    (linear_tf_m): Linear(in_features=22, out_features=256, bias=True)
    (linear_msa_m): Linear(in_features=49, out_features=256, bias=True)
    (linear_relpos): Linear(in_features=65, out_features=128, bias=True)
  )
  (recycling_embedder): RecyclingEmbedder(
    (linear): Linear(in_features=15, out_features=128, bias=True)
    (layer_norm_m): LayerNorm()
    (layer_norm_z): LayerNorm()
  )
  (template_embedder): TemplateEmbedder(
    (template_single_embedder): TemplateSingleEmbedder(
      (linear_1): Linear(in_features=57, out_features=256, bias=True)
      (relu): ReLU()
      (linear_2): Linear(in_features=256, out_features=256, bias=True)
    )
    (template_pair_embedder): TemplatePairEmbedder(
      (linear): Linear(in_features=88, out_features=64, bias=True)
    )
    (template_pair_stack): TemplatePairStack(
      (blocks): ModuleList(
        (0): TemplatePairStackBlock(
          (dropout_row): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
          (dropout_col): DropoutColumnwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=64, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=64, out_features=64, bias=False)
              (linear_k): Linear(in_features=64, out_features=64, bias=False)
              (linear_v): Linear(in_features=64, out_features=64, bias=False)
              (linear_o): Linear(in_features=64, out_features=64, bias=True)
              (linear_g): Linear(in_features=64, out_features=64, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttentionEndingNode(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=64, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=64, out_features=64, bias=False)
              (linear_k): Linear(in_features=64, out_features=64, bias=False)
              (linear_v): Linear(in_features=64, out_features=64, bias=False)
              (linear_o): Linear(in_features=64, out_features=64, bias=True)
              (linear_g): Linear(in_features=64, out_features=64, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_z): Linear(in_features=64, out_features=64, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_z): Linear(in_features=64, out_features=64, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=64, out_features=128, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=128, out_features=64, bias=True)
          )
        )
        (1): TemplatePairStackBlock(
          (dropout_row): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
          (dropout_col): DropoutColumnwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=64, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=64, out_features=64, bias=False)
              (linear_k): Linear(in_features=64, out_features=64, bias=False)
              (linear_v): Linear(in_features=64, out_features=64, bias=False)
              (linear_o): Linear(in_features=64, out_features=64, bias=True)
              (linear_g): Linear(in_features=64, out_features=64, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttentionEndingNode(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=64, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=64, out_features=64, bias=False)
              (linear_k): Linear(in_features=64, out_features=64, bias=False)
              (linear_v): Linear(in_features=64, out_features=64, bias=False)
              (linear_o): Linear(in_features=64, out_features=64, bias=True)
              (linear_g): Linear(in_features=64, out_features=64, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_z): Linear(in_features=64, out_features=64, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_z): Linear(in_features=64, out_features=64, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)
            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=64, out_features=128, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=128, out_features=64, bias=True)
          )
        )
      )
      (layer_norm): LayerNorm()
    )
    (template_pointwise_att): TemplatePointwiseAttention(
      (mha): Attention(
        (linear_q): Linear(in_features=128, out_features=64, bias=False)
        (linear_k): Linear(in_features=64, out_features=64, bias=False)
        (linear_v): Linear(in_features=64, out_features=64, bias=False)
        (linear_o): Linear(in_features=64, out_features=128, bias=True)
        (sigmoid): Sigmoid()
      )
    )
  )
  (extra_msa_embedder): ExtraMSAEmbedder(
    (linear): Linear(in_features=25, out_features=64, bias=True)
  )
  (extra_msa_stack): ExtraMSAStack(
    (blocks): ModuleList(
      (0): ExtraMSABlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=64, bias=False)
            (linear_v): Linear(in_features=64, out_features=64, bias=False)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=64, out_features=256, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=256, out_features=64, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=64, out_features=32, bias=True)
          (linear_2): Linear(in_features=64, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnGlobalAttention(
          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (global_attention): GlobalAttention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=8, bias=False)
            (linear_v): Linear(in_features=64, out_features=8, bias=False)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
      )
      (1): ExtraMSABlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=64, bias=False)
            (linear_v): Linear(in_features=64, out_features=64, bias=False)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=64, out_features=256, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=256, out_features=64, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=64, out_features=32, bias=True)
          (linear_2): Linear(in_features=64, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnGlobalAttention(
          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (global_attention): GlobalAttention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=8, bias=False)
            (linear_v): Linear(in_features=64, out_features=8, bias=False)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
      )
      (2): ExtraMSABlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=64, bias=False)
            (linear_v): Linear(in_features=64, out_features=64, bias=False)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=64, out_features=256, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=256, out_features=64, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=64, out_features=32, bias=True)
          (linear_2): Linear(in_features=64, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnGlobalAttention(
          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (global_attention): GlobalAttention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=8, bias=False)
            (linear_v): Linear(in_features=64, out_features=8, bias=False)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
      )
      (3): ExtraMSABlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=64, bias=False)
            (linear_v): Linear(in_features=64, out_features=64, bias=False)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=64, out_features=256, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=256, out_features=64, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=64, out_features=32, bias=True)
          (linear_2): Linear(in_features=64, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnGlobalAttention(
          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (global_attention): GlobalAttention(
            (linear_q): Linear(in_features=64, out_features=64, bias=False)
            (linear_k): Linear(in_features=64, out_features=8, bias=False)
            (linear_v): Linear(in_features=64, out_features=8, bias=False)
            (linear_g): Linear(in_features=64, out_features=64, bias=True)
            (linear_o): Linear(in_features=64, out_features=64, bias=True)
            (sigmoid): Sigmoid()
          )
        )
      )
    )
  )
  (evoformer): EvoformerStack(
    (blocks): ModuleList(
      (0): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (1): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (2): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (3): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (4): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (5): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (6): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (7): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (8): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (9): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (10): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (11): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (12): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (13): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (14): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (15): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (16): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (17): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (18): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (19): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (20): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (21): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (22): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (23): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (24): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (25): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (26): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (27): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (28): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (29): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (30): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (31): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (32): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (33): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (34): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (35): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (36): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (37): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (38): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (39): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (40): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (41): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (42): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (43): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (44): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (45): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (46): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
      (47): EvoformerBlock(
        (msa_att_row): MSARowAttentionWithPairBias(
          (layer_norm_m): LayerNorm()
          (layer_norm_z): LayerNorm()
          (linear_z): Linear(in_features=128, out_features=8, bias=False)
          (mha): Attention(
            (linear_q): Linear(in_features=256, out_features=256, bias=False)
            (linear_k): Linear(in_features=256, out_features=256, bias=False)
            (linear_v): Linear(in_features=256, out_features=256, bias=False)
            (linear_o): Linear(in_features=256, out_features=256, bias=True)
            (linear_g): Linear(in_features=256, out_features=256, bias=True)
            (sigmoid): Sigmoid()
          )
        )
        (msa_dropout_layer): DropoutRowwise(
          (dropout): Dropout(p=0.15, inplace=False)
        )
        (msa_transition): MSATransition(
          (layer_norm): LayerNorm()
          (linear_1): Linear(in_features=256, out_features=1024, bias=True)
          (relu): ReLU()
          (linear_2): Linear(in_features=1024, out_features=256, bias=True)
        )
        (outer_product_mean): OuterProductMean(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear_1): Linear(in_features=256, out_features=32, bias=True)
          (linear_2): Linear(in_features=256, out_features=32, bias=True)
          (linear_out): Linear(in_features=1024, out_features=128, bias=True)
        )
        (pair_stack): PairStack(
          (tri_mul_out): TriangleMultiplicationOutgoing(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_mul_in): TriangleMultiplicationIncoming(
            (linear_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_z): Linear(in_features=128, out_features=128, bias=True)
            (layer_norm_in): LayerNorm()
            (layer_norm_out): LayerNorm()
            (sigmoid): Sigmoid()
            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)
            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)
          )
          (tri_att_start): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (tri_att_end): TriangleAttention(
            (layer_norm): LayerNorm()
            (linear): Linear(in_features=128, out_features=4, bias=False)
            (mha): Attention(
              (linear_q): Linear(in_features=128, out_features=128, bias=False)
              (linear_k): Linear(in_features=128, out_features=128, bias=False)
              (linear_v): Linear(in_features=128, out_features=128, bias=False)
              (linear_o): Linear(in_features=128, out_features=128, bias=True)
              (linear_g): Linear(in_features=128, out_features=128, bias=True)
              (sigmoid): Sigmoid()
            )
          )
          (pair_transition): PairTransition(
            (layer_norm): LayerNorm()
            (linear_1): Linear(in_features=128, out_features=512, bias=True)
            (relu): ReLU()
            (linear_2): Linear(in_features=512, out_features=128, bias=True)
          )
          (ps_dropout_row_layer): DropoutRowwise(
            (dropout): Dropout(p=0.25, inplace=False)
          )
        )
        (msa_att_col): MSAColumnAttention(
          (_msa_att): MSAAttention(
            (layer_norm_m): LayerNorm()
            (mha): Attention(
              (linear_q): Linear(in_features=256, out_features=256, bias=False)
              (linear_k): Linear(in_features=256, out_features=256, bias=False)
              (linear_v): Linear(in_features=256, out_features=256, bias=False)
              (linear_o): Linear(in_features=256, out_features=256, bias=True)
              (linear_g): Linear(in_features=256, out_features=256, bias=True)
              (sigmoid): Sigmoid()
            )
          )
        )
      )
    )
    (linear): Linear(in_features=256, out_features=384, bias=True)
  )
  (structure_module): StructureModule(
    (layer_norm_s): LayerNorm()
    (layer_norm_z): LayerNorm()
    (linear_in): Linear(in_features=384, out_features=384, bias=True)
    (ipa): InvariantPointAttention(
      (linear_q): Linear(in_features=384, out_features=192, bias=True)
      (linear_q_points): PointProjection(
        (linear): Linear(in_features=384, out_features=144, bias=True)
      )
      (linear_kv): Linear(in_features=384, out_features=384, bias=True)
      (linear_kv_points): PointProjection(
        (linear): Linear(in_features=384, out_features=432, bias=True)
      )
      (linear_b): Linear(in_features=128, out_features=12, bias=True)
      (linear_out): Linear(in_features=2112, out_features=384, bias=True)
      (softmax): Softmax(dim=-1)
      (softplus): Softplus(beta=1, threshold=20)
    )
    (ipa_dropout): Dropout(p=0.1, inplace=False)
    (layer_norm_ipa): LayerNorm()
    (transition): StructureModuleTransition(
      (layers): ModuleList(
        (0): StructureModuleTransitionLayer(
          (linear_1): Linear(in_features=384, out_features=384, bias=True)
          (linear_2): Linear(in_features=384, out_features=384, bias=True)
          (linear_3): Linear(in_features=384, out_features=384, bias=True)
          (relu): ReLU()
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_norm): LayerNorm()
    )
    (bb_update): BackboneUpdate(
      (linear): Linear(in_features=384, out_features=6, bias=True)
    )
    (angle_resnet): AngleResnet(
      (linear_in): Linear(in_features=384, out_features=128, bias=True)
      (linear_initial): Linear(in_features=384, out_features=128, bias=True)
      (layers): ModuleList(
        (0): AngleResnetBlock(
          (linear_1): Linear(in_features=128, out_features=128, bias=True)
          (linear_2): Linear(in_features=128, out_features=128, bias=True)
          (relu): ReLU()
        )
        (1): AngleResnetBlock(
          (linear_1): Linear(in_features=128, out_features=128, bias=True)
          (linear_2): Linear(in_features=128, out_features=128, bias=True)
          (relu): ReLU()
        )
      )
      (linear_out): Linear(in_features=128, out_features=14, bias=True)
      (relu): ReLU()
    )
  )
  (aux_heads): AuxiliaryHeads(
    (plddt): PerResidueLDDTCaPredictor(
      (layer_norm): LayerNorm()
      (linear_1): Linear(in_features=384, out_features=128, bias=True)
      (linear_2): Linear(in_features=128, out_features=128, bias=True)
      (linear_3): Linear(in_features=128, out_features=50, bias=True)
      (relu): ReLU()
    )
    (distogram): DistogramHead(
      (linear): Linear(in_features=128, out_features=64, bias=True)
    )
    (masked_msa): MaskedMSAHead(
      (linear): Linear(in_features=256, out_features=23, bias=True)
    )
    (experimentally_resolved): ExperimentallyResolvedHead(
      (linear): Linear(in_features=384, out_features=37, bias=True)
    )
    (tm): TMScoreHead(
      (linear): Linear(in_features=128, out_features=64, bias=True)
    )
  )
)






batch before: {'aatype': tensor([[18, 18, 18, 18],
        [ 3,  3,  3,  3],
        [18, 18, 18, 18],
        [13, 13, 13, 13],
        [ 5,  5,  5,  5],
        [13, 13, 13, 13],
        [16, 16, 16, 16],
        [ 5,  5,  5,  5],
        [ 5,  5,  5,  5],
        [18, 18, 18, 18],
        [ 5,  5,  5,  5],
        [10, 10, 10, 10],
        [ 0,  0,  0,  0],
        [19, 19, 19, 19],
        [ 4,  4,  4,  4],
        [ 2,  2,  2,  2],
        [15, 15, 15, 15],
        [ 2,  2,  2,  2],
        [ 1,  1,  1,  1],
        [16, 16, 16, 16],
        [10, 10, 10, 10],
        [ 4,  4,  4,  4],
        [11, 11, 11, 11],
        [ 3,  3,  3,  3],
        [14, 14, 14, 14],
        [14, 14, 14, 14],
        [ 3,  3,  3,  3],
        [11, 11, 11, 11],
        [10, 10, 10, 10],
        [13, 13, 13, 13],
        [16, 16, 16, 16],
        [19, 19, 19, 19],
        [ 8,  8,  8,  8],
        [ 7,  7,  7,  7],
        [10, 10, 10, 10],
        [17, 17, 17, 17],
        [14, 14, 14, 14],
        [15, 15, 15, 15],
        [ 2,  2,  2,  2],
        [12, 12, 12, 12],
        [19, 19, 19, 19],
        [ 7,  7,  7,  7],
        [14, 14, 14, 14],
        [ 3,  3,  3,  3],
        [14, 14, 14, 14],
        [15, 15, 15, 15],
        [11, 11, 11, 11],
        [ 4,  4,  4,  4],
        [14, 14, 14, 14],
        [ 9,  9,  9,  9],
        [11, 11, 11, 11],
        [ 2,  2,  2,  2],
        [ 9,  9,  9,  9],
        [ 1,  1,  1,  1],
        [11, 11, 11, 11],
        [ 1,  1,  1,  1],
        [ 6,  6,  6,  6],
        [11, 11, 11, 11],
        [10, 10, 10, 10],
        [10, 10, 10, 10],
        [ 6,  6,  6,  6],
        [ 8,  8,  8,  8],
        [ 5,  5,  5,  5],
        [10, 10, 10, 10],
        [ 6,  6,  6,  6],
        [ 9,  9,  9,  9],
        [ 9,  9,  9,  9],
        [17, 17, 17, 17],
        [14, 14, 14, 14],
        [ 2,  2,  2,  2],
        [19, 19, 19, 19],
        [13, 13, 13, 13],
        [ 3,  3,  3,  3],
        [ 1,  1,  1,  1],
        [16, 16, 16, 16],
        [11, 11, 11, 11],
        [ 2,  2,  2,  2],
        [ 2,  2,  2,  2],
        [10, 10, 10, 10],
        [13, 13, 13, 13],
        [17, 17, 17, 17],
        [ 3,  3,  3,  3],
        [11, 11, 11, 11],
        [ 6,  6,  6,  6],
        [17, 17, 17, 17],
        [12, 12, 12, 12],
        [11, 11, 11, 11],
        [ 8,  8,  8,  8],
        [ 7,  7,  7,  7],
        [15, 15, 15, 15],
        [ 4,  4,  4,  4],
        [ 7,  7,  7,  7],
        [18, 18, 18, 18],
        [14, 14, 14, 14],
        [16, 16, 16, 16],
        [ 9,  9,  9,  9],
        [ 3,  3,  3,  3],
        [ 2,  2,  2,  2],
        [ 6,  6,  6,  6],
        [ 2,  2,  2,  2],
        [ 8,  8,  8,  8],
        [18, 18, 18, 18],
        [13, 13, 13, 13],
        [ 6,  6,  6,  6],
        [16, 16, 16, 16],
        [19, 19, 19, 19],
        [ 9,  9,  9,  9],
        [11, 11, 11, 11],
        [12, 12, 12, 12],
        [18, 18, 18, 18],
        [ 9,  9,  9,  9],
        [15, 15, 15, 15],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [ 5,  5,  5,  5],
        [ 2,  2,  2,  2],
        [19, 19, 19, 19],
        [15, 15, 15, 15],
        [ 1,  1,  1,  1],
        [ 9,  9,  9,  9],
        [10, 10, 10, 10],
        [15, 15, 15, 15],
        [11, 11, 11, 11],
        [ 0,  0,  0,  0],
        [11, 11, 11, 11],
        [ 9,  9,  9,  9],
        [ 6,  6,  6,  6],
        [14, 14, 14, 14],
        [ 3,  3,  3,  3],
        [ 7,  7,  7,  7],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [ 1,  1,  1,  1],
        [ 0,  0,  0,  0],
        [10, 10, 10, 10],
        [10, 10, 10, 10],
        [ 3,  3,  3,  3],
        [ 9,  9,  9,  9],
        [ 6,  6,  6,  6],
        [ 2,  2,  2,  2],
        [ 0,  0,  0,  0],
        [ 9,  9,  9,  9],
        [ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 7,  7,  7,  7],
        [ 0,  0,  0,  0],
        [ 3,  3,  3,  3],
        [ 2,  2,  2,  2],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [14, 14, 14, 14],
        [11, 11, 11, 11],
        [10, 10, 10, 10],
        [11, 11, 11, 11],
        [ 4,  4,  4,  4],
        [ 5,  5,  5,  5],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [ 7,  7,  7,  7],
        [16, 16, 16, 16],
        [16, 16, 16, 16],
        [16, 16, 16, 16],
        [ 6,  6,  6,  6],
        [10, 10, 10, 10],
        [19, 19, 19, 19],
        [ 6,  6,  6,  6],
        [ 9,  9,  9,  9],
        [16, 16, 16, 16],
        [10, 10, 10, 10],
        [ 4,  4,  4,  4],
        [15, 15, 15, 15],
        [ 3,  3,  3,  3],
        [11, 11, 11, 11],
        [15, 15, 15, 15],
        [ 7,  7,  7,  7],
        [ 6,  6,  6,  6],
        [ 8,  8,  8,  8],
        [13, 13, 13, 13],
        [ 9,  9,  9,  9],
        [ 3,  3,  3,  3],
        [ 4,  4,  4,  4],
        [14, 14, 14, 14],
        [ 8,  8,  8,  8],
        [14, 14, 14, 14],
        [13, 13, 13, 13],
        [ 6,  6,  6,  6],
        [14, 14, 14, 14],
        [ 9,  9,  9,  9],
        [15, 15, 15, 15],
        [14, 14, 14, 14],
        [ 8,  8,  8,  8],
        [18, 18, 18, 18],
        [ 4,  4,  4,  4],
        [14, 14, 14, 14],
        [16, 16, 16, 16],
        [ 2,  2,  2,  2],
        [ 2,  2,  2,  2],
        [ 9,  9,  9,  9],
        [11, 11, 11, 11],
        [18, 18, 18, 18]], device='cuda:0'), 'residue_index': tensor([[  0,   0,   0,   0],
        [  1,   1,   1,   1],
        [  2,   2,   2,   2],
        [  3,   3,   3,   3],
        [  4,   4,   4,   4],
        [  5,   5,   5,   5],
        [  6,   6,   6,   6],
        [  7,   7,   7,   7],
        [  8,   8,   8,   8],
        [  9,   9,   9,   9],
        [ 10,  10,  10,  10],
        [ 11,  11,  11,  11],
        [ 12,  12,  12,  12],
        [ 13,  13,  13,  13],
        [ 14,  14,  14,  14],
        [ 15,  15,  15,  15],
        [ 16,  16,  16,  16],
        [ 17,  17,  17,  17],
        [ 18,  18,  18,  18],
        [ 19,  19,  19,  19],
        [ 20,  20,  20,  20],
        [ 21,  21,  21,  21],
        [ 22,  22,  22,  22],
        [ 23,  23,  23,  23],
        [ 24,  24,  24,  24],
        [ 25,  25,  25,  25],
        [ 26,  26,  26,  26],
        [ 27,  27,  27,  27],
        [ 28,  28,  28,  28],
        [ 29,  29,  29,  29],
        [ 30,  30,  30,  30],
        [ 31,  31,  31,  31],
        [ 32,  32,  32,  32],
        [ 33,  33,  33,  33],
        [ 34,  34,  34,  34],
        [ 35,  35,  35,  35],
        [ 36,  36,  36,  36],
        [ 37,  37,  37,  37],
        [ 38,  38,  38,  38],
        [ 39,  39,  39,  39],
        [ 40,  40,  40,  40],
        [ 41,  41,  41,  41],
        [ 42,  42,  42,  42],
        [ 43,  43,  43,  43],
        [ 44,  44,  44,  44],
        [ 45,  45,  45,  45],
        [ 46,  46,  46,  46],
        [ 47,  47,  47,  47],
        [ 48,  48,  48,  48],
        [ 49,  49,  49,  49],
        [ 50,  50,  50,  50],
        [ 51,  51,  51,  51],
        [ 52,  52,  52,  52],
        [ 53,  53,  53,  53],
        [ 54,  54,  54,  54],
        [ 55,  55,  55,  55],
        [ 56,  56,  56,  56],
        [ 57,  57,  57,  57],
        [ 58,  58,  58,  58],
        [ 59,  59,  59,  59],
        [ 60,  60,  60,  60],
        [ 61,  61,  61,  61],
        [ 62,  62,  62,  62],
        [ 63,  63,  63,  63],
        [ 64,  64,  64,  64],
        [ 65,  65,  65,  65],
        [ 66,  66,  66,  66],
        [ 67,  67,  67,  67],
        [ 68,  68,  68,  68],
        [ 69,  69,  69,  69],
        [ 70,  70,  70,  70],
        [ 71,  71,  71,  71],
        [ 72,  72,  72,  72],
        [ 73,  73,  73,  73],
        [ 74,  74,  74,  74],
        [ 75,  75,  75,  75],
        [ 76,  76,  76,  76],
        [ 77,  77,  77,  77],
        [ 78,  78,  78,  78],
        [ 79,  79,  79,  79],
        [ 80,  80,  80,  80],
        [ 81,  81,  81,  81],
        [ 82,  82,  82,  82],
        [ 83,  83,  83,  83],
        [ 84,  84,  84,  84],
        [ 85,  85,  85,  85],
        [ 86,  86,  86,  86],
        [ 87,  87,  87,  87],
        [ 88,  88,  88,  88],
        [ 89,  89,  89,  89],
        [ 90,  90,  90,  90],
        [ 91,  91,  91,  91],
        [ 92,  92,  92,  92],
        [ 93,  93,  93,  93],
        [ 94,  94,  94,  94],
        [ 95,  95,  95,  95],
        [ 96,  96,  96,  96],
        [ 97,  97,  97,  97],
        [ 98,  98,  98,  98],
        [ 99,  99,  99,  99],
        [100, 100, 100, 100],
        [101, 101, 101, 101],
        [102, 102, 102, 102],
        [103, 103, 103, 103],
        [104, 104, 104, 104],
        [105, 105, 105, 105],
        [106, 106, 106, 106],
        [107, 107, 107, 107],
        [108, 108, 108, 108],
        [109, 109, 109, 109],
        [110, 110, 110, 110],
        [111, 111, 111, 111],
        [112, 112, 112, 112],
        [113, 113, 113, 113],
        [114, 114, 114, 114],
        [115, 115, 115, 115],
        [116, 116, 116, 116],
        [117, 117, 117, 117],
        [118, 118, 118, 118],
        [119, 119, 119, 119],
        [120, 120, 120, 120],
        [121, 121, 121, 121],
        [122, 122, 122, 122],
        [123, 123, 123, 123],
        [124, 124, 124, 124],
        [125, 125, 125, 125],
        [126, 126, 126, 126],
        [127, 127, 127, 127],
        [128, 128, 128, 128],
        [129, 129, 129, 129],
        [130, 130, 130, 130],
        [131, 131, 131, 131],
        [132, 132, 132, 132],
        [133, 133, 133, 133],
        [134, 134, 134, 134],
        [135, 135, 135, 135],
        [136, 136, 136, 136],
        [137, 137, 137, 137],
        [138, 138, 138, 138],
        [139, 139, 139, 139],
        [140, 140, 140, 140],
        [141, 141, 141, 141],
        [142, 142, 142, 142],
        [143, 143, 143, 143],
        [144, 144, 144, 144],
        [145, 145, 145, 145],
        [146, 146, 146, 146],
        [147, 147, 147, 147],
        [148, 148, 148, 148],
        [149, 149, 149, 149],
        [150, 150, 150, 150],
        [151, 151, 151, 151],
        [152, 152, 152, 152],
        [153, 153, 153, 153],
        [154, 154, 154, 154],
        [155, 155, 155, 155],
        [156, 156, 156, 156],
        [157, 157, 157, 157],
        [158, 158, 158, 158],
        [159, 159, 159, 159],
        [160, 160, 160, 160],
        [161, 161, 161, 161],
        [162, 162, 162, 162],
        [163, 163, 163, 163],
        [164, 164, 164, 164],
        [165, 165, 165, 165],
        [166, 166, 166, 166],
        [167, 167, 167, 167],
        [168, 168, 168, 168],
        [169, 169, 169, 169],
        [170, 170, 170, 170],
        [171, 171, 171, 171],
        [172, 172, 172, 172],
        [173, 173, 173, 173],
        [174, 174, 174, 174],
        [175, 175, 175, 175],
        [176, 176, 176, 176],
        [177, 177, 177, 177],
        [178, 178, 178, 178],
        [179, 179, 179, 179],
        [180, 180, 180, 180],
        [181, 181, 181, 181],
        [182, 182, 182, 182],
        [183, 183, 183, 183],
        [184, 184, 184, 184],
        [185, 185, 185, 185],
        [186, 186, 186, 186],
        [187, 187, 187, 187],
        [188, 188, 188, 188],
        [189, 189, 189, 189],
        [190, 190, 190, 190],
        [191, 191, 191, 191],
        [192, 192, 192, 192],
        [193, 193, 193, 193],
        [194, 194, 194, 194],
        [195, 195, 195, 195],
        [196, 196, 196, 196],
        [197, 197, 197, 197],
        [198, 198, 198, 198],
        [199, 199, 199, 199]], device='cuda:0', dtype=torch.int32), 'seq_length': tensor([200, 200, 200, 200], device='cuda:0', dtype=torch.int32), 'template_aatype': tensor([[[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]]], device='cuda:0'), 'template_all_atom_masks': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0'), 'template_all_atom_positions': tensor([[[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]]], device='cuda:0'), 'template_sum_probs': tensor([[[0., 0., 0., 0.]],

        [[0., 0., 0., 0.]],

        [[0., 0., 0., 0.]],

        [[0., 0., 0., 0.]]], device='cuda:0'), 'is_distillation': tensor([0., 0., 0., 0.], device='cuda:0'), 'seq_mask': tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]], device='cuda:0'), 'msa_mask': tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        ...,

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]]], device='cuda:0'), 'msa_row_mask': tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        ...,
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]], device='cuda:0'), 'random_crop_to_size_seed': tensor([[-2040408457, -2040408457, -2040408457, -2040408457],
        [  914515801,   914515801,   914515801,   914515801]], device='cuda:0',
       dtype=torch.int32), 'template_mask': tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]], device='cuda:0'), 'template_pseudo_beta': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0'), 'template_pseudo_beta_mask': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'atom14_atom_exists': tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 0., 1.,  ..., 0., 0., 1.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), 'residx_atom14_to_atom37': tensor([[ 0,  0,  0,  ...,  0,  0,  0],
        [ 1,  1,  1,  ...,  1,  1,  1],
        [ 2,  2,  2,  ...,  2,  2,  2],
        ...,
        [31,  0, 31,  ...,  0,  0, 31],
        [ 0,  0,  0,  ...,  0,  0,  0],
        [ 0,  0,  0,  ...,  0,  0,  0]], device='cuda:0', dtype=torch.int32), 'residx_atom37_to_atom14': tensor([[[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        ...,

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [8, 8, 8, 8],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]]], device='cuda:0'), 'atom37_atom_exists': tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [1., 1., 1., 1.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'extra_msa': tensor([[[18, 21, 21, 13],
         [ 3, 21, 21,  3],
         [18, 21, 21, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[13, 21, 13, 18],
         [ 3, 21,  3,  3],
         [13, 21, 18, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[13, 21, 21, 21],
         [ 3, 21, 21,  3],
         [13, 21, 21, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        ...,

        [[ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         ...,
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0]],

        [[ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         ...,
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0]],

        [[ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         ...,
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0]]], device='cuda:0'), 'extra_msa_mask': tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'extra_msa_row_mask': tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        ...,
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]], device='cuda:0'), 'bert_mask': tensor([[[0., 0., 0., 0.],
         [1., 1., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [1., 0., 0., 0.],
         [1., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 1., 1.],
         [1., 1., 0., 1.],
         [1., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [1., 1., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 1., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 1.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[1., 0., 0., 0.],
         [0., 0., 1., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [1., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 1.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 1., 0., 0.],
         ...,
         [0., 0., 1., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'true_msa': tensor([[[18, 18, 18, 18],
         [ 3,  3,  3,  3],
         [18, 18, 18, 18],
         ...,
         [ 9,  9,  9,  9],
         [11, 11, 11, 11],
         [18, 18, 18, 18]],

        [[18, 21, 13, 18],
         [ 3, 21,  3,  3],
         [13, 21, 13, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[13, 21, 13, 13],
         [ 3,  3,  3,  3],
         [13, 18, 13, 13],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        ...,

        [[13, 21, 18, 21],
         [ 3, 21, 19,  3],
         [13, 21, 18, 19],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[21, 18, 21, 21],
         [21,  3, 21, 21],
         [21, 13, 21, 21],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[18, 21, 21, 21],
         [ 3, 21, 21, 21],
         [13, 21, 21, 21],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]]], device='cuda:0', dtype=torch.int32), 'extra_has_deletion': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'extra_deletion_value': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'msa_feat': tensor([[[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5469, 0.8462, 0.6944, 0.8667],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.4844, 0.7692, 0.6667, 0.7333],
          [0.0000, 0.0385, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.4531, 0.7692, 0.6667, 0.6000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.8906, 0.7692, 0.7778, 0.8667],
          [0.0156, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.8906, 0.7692, 0.8056, 0.8667],
          [0.0156, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.9062, 0.8077, 0.8056, 0.8667],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0833, 1.0000, 0.1429, 0.3333],
          [0.0000, 0.0000, 0.1429, 0.3333],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0833, 0.5000, 0.1429, 0.3333],
          [0.0833, 0.5000, 0.0000, 0.3333],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0833, 1.0000, 0.0000, 0.3333],
          [0.0833, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.9167, 0.5000, 1.0000, 1.0000],
          [0.0833, 0.5000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0000, 0.4286, 0.2632, 0.0000],
          [0.0000, 0.0476, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0000, 0.0476, 0.2105, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0000, 0.0476, 0.2105, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        ...,


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 0.4000, 0.7895],
          [0.5000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 0.4000, 0.0526],
          [0.0000, 0.0000, 0.2000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 0.2000, 0.0526],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 0.9474],
          [0.0000, 0.0000, 0.0000, 0.0526],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 0.1250, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 0.1250, 0.8333, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 0.1250, 0.6667, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.9737, 1.0000, 1.0000, 1.0000],
          [0.0263, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 1.0000, 0.8571],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 0.0000, 1.0000, 0.8571],
          [0.0000, 1.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 0.8571, 1.0000],
          [0.0000, 0.0000, 0.1429, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]]], device='cuda:0'), 'target_feat': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'no_recycling_iters': tensor([3., 3., 3., 3.], device='cuda:0', dtype=torch.float64), 'template_all_atom_mask': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0'), 'template_torsion_angles_sin_cos': tensor([[[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]]], device='cuda:0'), 'template_alt_torsion_angles_sin_cos': tensor([[[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]]], device='cuda:0'), 'template_torsion_angles_mask': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0')}






batch after: {'aatype': tensor([[18, 18, 18, 18],
        [ 3,  3,  3,  3],
        [18, 18, 18, 18],
        [13, 13, 13, 13],
        [ 5,  5,  5,  5],
        [13, 13, 13, 13],
        [16, 16, 16, 16],
        [ 5,  5,  5,  5],
        [ 5,  5,  5,  5],
        [18, 18, 18, 18],
        [ 5,  5,  5,  5],
        [10, 10, 10, 10],
        [ 0,  0,  0,  0],
        [19, 19, 19, 19],
        [ 4,  4,  4,  4],
        [ 2,  2,  2,  2],
        [15, 15, 15, 15],
        [ 2,  2,  2,  2],
        [ 1,  1,  1,  1],
        [16, 16, 16, 16],
        [10, 10, 10, 10],
        [ 4,  4,  4,  4],
        [11, 11, 11, 11],
        [ 3,  3,  3,  3],
        [14, 14, 14, 14],
        [14, 14, 14, 14],
        [ 3,  3,  3,  3],
        [11, 11, 11, 11],
        [10, 10, 10, 10],
        [13, 13, 13, 13],
        [16, 16, 16, 16],
        [19, 19, 19, 19],
        [ 8,  8,  8,  8],
        [ 7,  7,  7,  7],
        [10, 10, 10, 10],
        [17, 17, 17, 17],
        [14, 14, 14, 14],
        [15, 15, 15, 15],
        [ 2,  2,  2,  2],
        [12, 12, 12, 12],
        [19, 19, 19, 19],
        [ 7,  7,  7,  7],
        [14, 14, 14, 14],
        [ 3,  3,  3,  3],
        [14, 14, 14, 14],
        [15, 15, 15, 15],
        [11, 11, 11, 11],
        [ 4,  4,  4,  4],
        [14, 14, 14, 14],
        [ 9,  9,  9,  9],
        [11, 11, 11, 11],
        [ 2,  2,  2,  2],
        [ 9,  9,  9,  9],
        [ 1,  1,  1,  1],
        [11, 11, 11, 11],
        [ 1,  1,  1,  1],
        [ 6,  6,  6,  6],
        [11, 11, 11, 11],
        [10, 10, 10, 10],
        [10, 10, 10, 10],
        [ 6,  6,  6,  6],
        [ 8,  8,  8,  8],
        [ 5,  5,  5,  5],
        [10, 10, 10, 10],
        [ 6,  6,  6,  6],
        [ 9,  9,  9,  9],
        [ 9,  9,  9,  9],
        [17, 17, 17, 17],
        [14, 14, 14, 14],
        [ 2,  2,  2,  2],
        [19, 19, 19, 19],
        [13, 13, 13, 13],
        [ 3,  3,  3,  3],
        [ 1,  1,  1,  1],
        [16, 16, 16, 16],
        [11, 11, 11, 11],
        [ 2,  2,  2,  2],
        [ 2,  2,  2,  2],
        [10, 10, 10, 10],
        [13, 13, 13, 13],
        [17, 17, 17, 17],
        [ 3,  3,  3,  3],
        [11, 11, 11, 11],
        [ 6,  6,  6,  6],
        [17, 17, 17, 17],
        [12, 12, 12, 12],
        [11, 11, 11, 11],
        [ 8,  8,  8,  8],
        [ 7,  7,  7,  7],
        [15, 15, 15, 15],
        [ 4,  4,  4,  4],
        [ 7,  7,  7,  7],
        [18, 18, 18, 18],
        [14, 14, 14, 14],
        [16, 16, 16, 16],
        [ 9,  9,  9,  9],
        [ 3,  3,  3,  3],
        [ 2,  2,  2,  2],
        [ 6,  6,  6,  6],
        [ 2,  2,  2,  2],
        [ 8,  8,  8,  8],
        [18, 18, 18, 18],
        [13, 13, 13, 13],
        [ 6,  6,  6,  6],
        [16, 16, 16, 16],
        [19, 19, 19, 19],
        [ 9,  9,  9,  9],
        [11, 11, 11, 11],
        [12, 12, 12, 12],
        [18, 18, 18, 18],
        [ 9,  9,  9,  9],
        [15, 15, 15, 15],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [ 5,  5,  5,  5],
        [ 2,  2,  2,  2],
        [19, 19, 19, 19],
        [15, 15, 15, 15],
        [ 1,  1,  1,  1],
        [ 9,  9,  9,  9],
        [10, 10, 10, 10],
        [15, 15, 15, 15],
        [11, 11, 11, 11],
        [ 0,  0,  0,  0],
        [11, 11, 11, 11],
        [ 9,  9,  9,  9],
        [ 6,  6,  6,  6],
        [14, 14, 14, 14],
        [ 3,  3,  3,  3],
        [ 7,  7,  7,  7],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [ 1,  1,  1,  1],
        [ 0,  0,  0,  0],
        [10, 10, 10, 10],
        [10, 10, 10, 10],
        [ 3,  3,  3,  3],
        [ 9,  9,  9,  9],
        [ 6,  6,  6,  6],
        [ 2,  2,  2,  2],
        [ 0,  0,  0,  0],
        [ 9,  9,  9,  9],
        [ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 7,  7,  7,  7],
        [ 0,  0,  0,  0],
        [ 3,  3,  3,  3],
        [ 2,  2,  2,  2],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [14, 14, 14, 14],
        [11, 11, 11, 11],
        [10, 10, 10, 10],
        [11, 11, 11, 11],
        [ 4,  4,  4,  4],
        [ 5,  5,  5,  5],
        [11, 11, 11, 11],
        [11, 11, 11, 11],
        [ 7,  7,  7,  7],
        [16, 16, 16, 16],
        [16, 16, 16, 16],
        [16, 16, 16, 16],
        [ 6,  6,  6,  6],
        [10, 10, 10, 10],
        [19, 19, 19, 19],
        [ 6,  6,  6,  6],
        [ 9,  9,  9,  9],
        [16, 16, 16, 16],
        [10, 10, 10, 10],
        [ 4,  4,  4,  4],
        [15, 15, 15, 15],
        [ 3,  3,  3,  3],
        [11, 11, 11, 11],
        [15, 15, 15, 15],
        [ 7,  7,  7,  7],
        [ 6,  6,  6,  6],
        [ 8,  8,  8,  8],
        [13, 13, 13, 13],
        [ 9,  9,  9,  9],
        [ 3,  3,  3,  3],
        [ 4,  4,  4,  4],
        [14, 14, 14, 14],
        [ 8,  8,  8,  8],
        [14, 14, 14, 14],
        [13, 13, 13, 13],
        [ 6,  6,  6,  6],
        [14, 14, 14, 14],
        [ 9,  9,  9,  9],
        [15, 15, 15, 15],
        [14, 14, 14, 14],
        [ 8,  8,  8,  8],
        [18, 18, 18, 18],
        [ 4,  4,  4,  4],
        [14, 14, 14, 14],
        [16, 16, 16, 16],
        [ 2,  2,  2,  2],
        [ 2,  2,  2,  2],
        [ 9,  9,  9,  9],
        [11, 11, 11, 11],
        [18, 18, 18, 18]], device='cuda:0'), 'residue_index': tensor([[  0,   0,   0,   0],
        [  1,   1,   1,   1],
        [  2,   2,   2,   2],
        [  3,   3,   3,   3],
        [  4,   4,   4,   4],
        [  5,   5,   5,   5],
        [  6,   6,   6,   6],
        [  7,   7,   7,   7],
        [  8,   8,   8,   8],
        [  9,   9,   9,   9],
        [ 10,  10,  10,  10],
        [ 11,  11,  11,  11],
        [ 12,  12,  12,  12],
        [ 13,  13,  13,  13],
        [ 14,  14,  14,  14],
        [ 15,  15,  15,  15],
        [ 16,  16,  16,  16],
        [ 17,  17,  17,  17],
        [ 18,  18,  18,  18],
        [ 19,  19,  19,  19],
        [ 20,  20,  20,  20],
        [ 21,  21,  21,  21],
        [ 22,  22,  22,  22],
        [ 23,  23,  23,  23],
        [ 24,  24,  24,  24],
        [ 25,  25,  25,  25],
        [ 26,  26,  26,  26],
        [ 27,  27,  27,  27],
        [ 28,  28,  28,  28],
        [ 29,  29,  29,  29],
        [ 30,  30,  30,  30],
        [ 31,  31,  31,  31],
        [ 32,  32,  32,  32],
        [ 33,  33,  33,  33],
        [ 34,  34,  34,  34],
        [ 35,  35,  35,  35],
        [ 36,  36,  36,  36],
        [ 37,  37,  37,  37],
        [ 38,  38,  38,  38],
        [ 39,  39,  39,  39],
        [ 40,  40,  40,  40],
        [ 41,  41,  41,  41],
        [ 42,  42,  42,  42],
        [ 43,  43,  43,  43],
        [ 44,  44,  44,  44],
        [ 45,  45,  45,  45],
        [ 46,  46,  46,  46],
        [ 47,  47,  47,  47],
        [ 48,  48,  48,  48],
        [ 49,  49,  49,  49],
        [ 50,  50,  50,  50],
        [ 51,  51,  51,  51],
        [ 52,  52,  52,  52],
        [ 53,  53,  53,  53],
        [ 54,  54,  54,  54],
        [ 55,  55,  55,  55],
        [ 56,  56,  56,  56],
        [ 57,  57,  57,  57],
        [ 58,  58,  58,  58],
        [ 59,  59,  59,  59],
        [ 60,  60,  60,  60],
        [ 61,  61,  61,  61],
        [ 62,  62,  62,  62],
        [ 63,  63,  63,  63],
        [ 64,  64,  64,  64],
        [ 65,  65,  65,  65],
        [ 66,  66,  66,  66],
        [ 67,  67,  67,  67],
        [ 68,  68,  68,  68],
        [ 69,  69,  69,  69],
        [ 70,  70,  70,  70],
        [ 71,  71,  71,  71],
        [ 72,  72,  72,  72],
        [ 73,  73,  73,  73],
        [ 74,  74,  74,  74],
        [ 75,  75,  75,  75],
        [ 76,  76,  76,  76],
        [ 77,  77,  77,  77],
        [ 78,  78,  78,  78],
        [ 79,  79,  79,  79],
        [ 80,  80,  80,  80],
        [ 81,  81,  81,  81],
        [ 82,  82,  82,  82],
        [ 83,  83,  83,  83],
        [ 84,  84,  84,  84],
        [ 85,  85,  85,  85],
        [ 86,  86,  86,  86],
        [ 87,  87,  87,  87],
        [ 88,  88,  88,  88],
        [ 89,  89,  89,  89],
        [ 90,  90,  90,  90],
        [ 91,  91,  91,  91],
        [ 92,  92,  92,  92],
        [ 93,  93,  93,  93],
        [ 94,  94,  94,  94],
        [ 95,  95,  95,  95],
        [ 96,  96,  96,  96],
        [ 97,  97,  97,  97],
        [ 98,  98,  98,  98],
        [ 99,  99,  99,  99],
        [100, 100, 100, 100],
        [101, 101, 101, 101],
        [102, 102, 102, 102],
        [103, 103, 103, 103],
        [104, 104, 104, 104],
        [105, 105, 105, 105],
        [106, 106, 106, 106],
        [107, 107, 107, 107],
        [108, 108, 108, 108],
        [109, 109, 109, 109],
        [110, 110, 110, 110],
        [111, 111, 111, 111],
        [112, 112, 112, 112],
        [113, 113, 113, 113],
        [114, 114, 114, 114],
        [115, 115, 115, 115],
        [116, 116, 116, 116],
        [117, 117, 117, 117],
        [118, 118, 118, 118],
        [119, 119, 119, 119],
        [120, 120, 120, 120],
        [121, 121, 121, 121],
        [122, 122, 122, 122],
        [123, 123, 123, 123],
        [124, 124, 124, 124],
        [125, 125, 125, 125],
        [126, 126, 126, 126],
        [127, 127, 127, 127],
        [128, 128, 128, 128],
        [129, 129, 129, 129],
        [130, 130, 130, 130],
        [131, 131, 131, 131],
        [132, 132, 132, 132],
        [133, 133, 133, 133],
        [134, 134, 134, 134],
        [135, 135, 135, 135],
        [136, 136, 136, 136],
        [137, 137, 137, 137],
        [138, 138, 138, 138],
        [139, 139, 139, 139],
        [140, 140, 140, 140],
        [141, 141, 141, 141],
        [142, 142, 142, 142],
        [143, 143, 143, 143],
        [144, 144, 144, 144],
        [145, 145, 145, 145],
        [146, 146, 146, 146],
        [147, 147, 147, 147],
        [148, 148, 148, 148],
        [149, 149, 149, 149],
        [150, 150, 150, 150],
        [151, 151, 151, 151],
        [152, 152, 152, 152],
        [153, 153, 153, 153],
        [154, 154, 154, 154],
        [155, 155, 155, 155],
        [156, 156, 156, 156],
        [157, 157, 157, 157],
        [158, 158, 158, 158],
        [159, 159, 159, 159],
        [160, 160, 160, 160],
        [161, 161, 161, 161],
        [162, 162, 162, 162],
        [163, 163, 163, 163],
        [164, 164, 164, 164],
        [165, 165, 165, 165],
        [166, 166, 166, 166],
        [167, 167, 167, 167],
        [168, 168, 168, 168],
        [169, 169, 169, 169],
        [170, 170, 170, 170],
        [171, 171, 171, 171],
        [172, 172, 172, 172],
        [173, 173, 173, 173],
        [174, 174, 174, 174],
        [175, 175, 175, 175],
        [176, 176, 176, 176],
        [177, 177, 177, 177],
        [178, 178, 178, 178],
        [179, 179, 179, 179],
        [180, 180, 180, 180],
        [181, 181, 181, 181],
        [182, 182, 182, 182],
        [183, 183, 183, 183],
        [184, 184, 184, 184],
        [185, 185, 185, 185],
        [186, 186, 186, 186],
        [187, 187, 187, 187],
        [188, 188, 188, 188],
        [189, 189, 189, 189],
        [190, 190, 190, 190],
        [191, 191, 191, 191],
        [192, 192, 192, 192],
        [193, 193, 193, 193],
        [194, 194, 194, 194],
        [195, 195, 195, 195],
        [196, 196, 196, 196],
        [197, 197, 197, 197],
        [198, 198, 198, 198],
        [199, 199, 199, 199]], device='cuda:0', dtype=torch.int32), 'seq_length': tensor([200, 200, 200, 200], device='cuda:0', dtype=torch.int32), 'template_aatype': tensor([[[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]]], device='cuda:0'), 'template_all_atom_masks': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0'), 'template_all_atom_positions': tensor([[[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]]]]], device='cuda:0'), 'template_sum_probs': tensor([[[0., 0., 0., 0.]],

        [[0., 0., 0., 0.]],

        [[0., 0., 0., 0.]],

        [[0., 0., 0., 0.]]], device='cuda:0'), 'is_distillation': tensor([0., 0., 0., 0.], device='cuda:0'), 'seq_mask': tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]], device='cuda:0'), 'msa_mask': tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        ...,

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]]], device='cuda:0'), 'msa_row_mask': tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        ...,
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]], device='cuda:0'), 'random_crop_to_size_seed': tensor([[-2040408457, -2040408457, -2040408457, -2040408457],
        [  914515801,   914515801,   914515801,   914515801]], device='cuda:0',
       dtype=torch.int32), 'template_mask': tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]], device='cuda:0'), 'template_pseudo_beta': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0'), 'template_pseudo_beta_mask': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'atom14_atom_exists': tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 0., 1.,  ..., 0., 0., 1.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), 'residx_atom14_to_atom37': tensor([[ 0,  0,  0,  ...,  0,  0,  0],
        [ 1,  1,  1,  ...,  1,  1,  1],
        [ 2,  2,  2,  ...,  2,  2,  2],
        ...,
        [31,  0, 31,  ...,  0,  0, 31],
        [ 0,  0,  0,  ...,  0,  0,  0],
        [ 0,  0,  0,  ...,  0,  0,  0]], device='cuda:0', dtype=torch.int32), 'residx_atom37_to_atom14': tensor([[[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        ...,

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [8, 8, 8, 8],
         [0, 0, 0, 0]],

        [[0, 0, 0, 0],
         [1, 1, 1, 1],
         [2, 2, 2, 2],
         ...,
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]]], device='cuda:0'), 'atom37_atom_exists': tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [1., 1., 1., 1.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'extra_msa': tensor([[[18, 21, 21, 13],
         [ 3, 21, 21,  3],
         [18, 21, 21, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[13, 21, 13, 18],
         [ 3, 21,  3,  3],
         [13, 21, 18, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[13, 21, 21, 21],
         [ 3, 21, 21,  3],
         [13, 21, 21, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        ...,

        [[ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         ...,
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0]],

        [[ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         ...,
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0]],

        [[ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         ...,
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0],
         [ 0,  0,  0,  0]]], device='cuda:0'), 'extra_msa_mask': tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         ...,
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'extra_msa_row_mask': tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.],
        ...,
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]], device='cuda:0'), 'bert_mask': tensor([[[0., 0., 0., 0.],
         [1., 1., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [1., 0., 0., 0.],
         [1., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 1., 1.],
         [1., 1., 0., 1.],
         [1., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [1., 1., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 1., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 1.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[1., 0., 0., 0.],
         [0., 0., 1., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [1., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 1.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 1., 0., 0.],
         ...,
         [0., 0., 1., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'true_msa': tensor([[[18, 18, 18, 18],
         [ 3,  3,  3,  3],
         [18, 18, 18, 18],
         ...,
         [ 9,  9,  9,  9],
         [11, 11, 11, 11],
         [18, 18, 18, 18]],

        [[18, 21, 13, 18],
         [ 3, 21,  3,  3],
         [13, 21, 13, 18],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[13, 21, 13, 13],
         [ 3,  3,  3,  3],
         [13, 18, 13, 13],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        ...,

        [[13, 21, 18, 21],
         [ 3, 21, 19,  3],
         [13, 21, 18, 19],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[21, 18, 21, 21],
         [21,  3, 21, 21],
         [21, 13, 21, 21],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]],

        [[18, 21, 21, 21],
         [ 3, 21, 21, 21],
         [13, 21, 21, 21],
         ...,
         [21, 21, 21, 21],
         [21, 21, 21, 21],
         [21, 21, 21, 21]]], device='cuda:0', dtype=torch.int32), 'extra_has_deletion': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'extra_deletion_value': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'msa_feat': tensor([[[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5469, 0.8462, 0.6944, 0.8667],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.4844, 0.7692, 0.6667, 0.7333],
          [0.0000, 0.0385, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.4531, 0.7692, 0.6667, 0.6000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.8906, 0.7692, 0.7778, 0.8667],
          [0.0156, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.8906, 0.7692, 0.8056, 0.8667],
          [0.0156, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.9062, 0.8077, 0.8056, 0.8667],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0833, 1.0000, 0.1429, 0.3333],
          [0.0000, 0.0000, 0.1429, 0.3333],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0833, 0.5000, 0.1429, 0.3333],
          [0.0833, 0.5000, 0.0000, 0.3333],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0833, 1.0000, 0.0000, 0.3333],
          [0.0833, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.9167, 0.5000, 1.0000, 1.0000],
          [0.0833, 0.5000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0000, 0.4286, 0.2632, 0.0000],
          [0.0000, 0.0476, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0000, 0.0476, 0.2105, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.0000, 0.0476, 0.2105, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        ...,


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 0.4000, 0.7895],
          [0.5000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 0.4000, 0.0526],
          [0.0000, 0.0000, 0.2000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 0.2000, 0.0526],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 0.9474],
          [0.0000, 0.0000, 0.0000, 0.0526],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 0.1250, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 0.1250, 0.8333, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 0.1250, 0.6667, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.9737, 1.0000, 1.0000, 1.0000],
          [0.0263, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]],


        [[[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 1.0000, 1.0000, 0.8571],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [0.5000, 0.0000, 1.0000, 0.8571],
          [0.0000, 1.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         ...,

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 0.8571, 1.0000],
          [0.0000, 0.0000, 0.1429, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]],

         [[0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          ...,
          [1.0000, 1.0000, 1.0000, 1.0000],
          [0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000]]]], device='cuda:0'), 'target_feat': tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        ...,

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         ...,
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]], device='cuda:0'), 'no_recycling_iters': tensor([3., 3., 3., 3.], device='cuda:0', dtype=torch.float64), 'template_all_atom_mask': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0'), 'template_torsion_angles_sin_cos': tensor([[[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]]], device='cuda:0'), 'template_alt_torsion_angles_sin_cos': tensor([[[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]],



        [[[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         ...,


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]],


         [[[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[-0., -0., -0., -0.],
           [-0., -0., -0., -0.]],

          ...,

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]],

          [[0., 0., 0., 0.],
           [0., 0., 0., 0.]]]]], device='cuda:0'), 'template_torsion_angles_mask': tensor([[[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]],


        [[[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         ...,

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]],

         [[0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          ...,
          [0., 0., 0., 0.],
          [0., 0., 0., 0.],
          [0., 0., 0., 0.]]]], device='cuda:0')}






orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32
orig_dtype: torch.float32

Test(s) failed. Make sure you've installed all Python dependencies.
